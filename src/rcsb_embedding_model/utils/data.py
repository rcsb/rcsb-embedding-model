import os
import requests
import gzip
import torch
import logging

from requests import RequestException, ConnectTimeout
from io import StringIO, BytesIO

def collate_seq_embeddings(batch_list):
    """
    Pads the tensors in a batch to the same size.

    Args:
        batch_list (list of torch.Tensor): A list of samples, where each sample is a tensor of shape (sequence_length, embedding_dim).

    Returns:
        tuple: A tuple containing:
            - padded_batch (torch.Tensor): A tensor of shape (batch_size, max_seq_length, embedding_dim), where each sample is padded to the max sequence length.
            - mask_batch (torch.Tensor): A tensor of shape (batch_size, max_seq_length) where padded positions are marked as False.
    """
    if batch_list[0] is None:
        return None
    device = batch_list[0].device  # Get the device of the input tensors
    max_len = max(sample.size(0) for sample in batch_list)  # Determine the maximum sequence length
    dim = batch_list[0].size(1)  # Determine the embedding dimension
    batch_size = len(batch_list)  # Determine the batch size

    # Initialize tensors for the padded batch and masks on the same device as the input tensors
    padded_batch = torch.zeros((batch_size, max_len, dim), dtype=batch_list[0].dtype, device=device)
    mask_batch = torch.ones((batch_size, max_len), dtype=torch.bool, device=device)

    for i, sample in enumerate(batch_list):
        seq_len = sample.size(0)  # Get the length of the current sequence
        padded_batch[i, :seq_len] = sample  # Pad the sequence with zeros
        mask_batch[i, :seq_len] = False  # Set mask positions for the actual data to False

    return padded_batch, mask_batch


def stringio_from_url(url):
    return run_with_retries(
        __stringio_from_url,
        url,
        retries=5,
        delay=60,
        backoff=2,
        exceptions=(RequestException, IOError)
    )


def __stringio_from_url(url):
    try:
        response = requests.get(url, timeout=60)
        response.raise_for_status()
        data = response.content
        if url.endswith('.bcif.gz'):
            with gzip.GzipFile(fileobj=BytesIO(data), mode='rb') as gz:
                decompressed_data = gz.read()
                return BytesIO(decompressed_data)
        if url.endswith('.gz'):
            compressed = BytesIO(data)
            with gzip.open(compressed, 'rt') as f:
                return StringIO(f.read())
        else:
            return StringIO(response.text)
    except (RequestException, ConnectTimeout) as e:
        raise RequestException(f"Error fetching URL {url}: {e}")
    except (OSError, gzip.BadGzipFile) as e:
        raise IOError(f"Error decompressing gzip file {url}: {e}")


def concatenate_tensors(file_list, max_residues, dim=0):
    """
    Concatenates a list of tensors stored in individual files along a specified dimension.

    Args:
        file_list (list of str): List of file paths to tensor files.
        max_residues (int): Maximum number of residues allowed in the assembly
        dim (int): The dimension along which to concatenate the tensors. Default is 0.

    Returns:
        torch.Tensor: The concatenated tensor.
    """
    tensors = []
    total_residues = 0
    for file in file_list:
        try:
            tensor = torch.load(
                file,
                map_location=torch.device('cpu')
            )
            total_residues += tensor.shape[0]
            tensors.append(tensor)
        except Exception as e:
            import logging
            logging.getLogger(__name__).warning(f"Error loading tensor from {file}: {e}")
            continue
        if total_residues > max_residues:
            break
    if tensors and len(tensors) > 0:
        tensor_cat = torch.cat(tensors, dim=dim)
        return tensor_cat
    else:
        raise ValueError(f"No valid tensors were loaded to concatenate. {', '.join(file_list)}")

def adapt_csv_to_embedding_chain_stream(src_file, res_embedding_location):
    def __parse_row(row):
        r = row.split(",")
        return os.path.join(res_embedding_location, f"{r[0]}.{r[2]}.pt"), f"{r[0]}.{r[2]}"
    return tuple([__parse_row(r.strip()) for r in open(src_file) if len(r.split(",")) > 2])


# Generated by ChatGPT 5.1
import time
from typing import Callable, Iterable, Type, Any
def run_with_retries(
        func: Callable,
        *args,
        retries: int = 3,
        delay: float = 0.5,
        backoff: float = 1.0,
        exceptions: Iterable[Type[BaseException]] = (Exception,),
        **kwargs: Any
) -> Any:
    """
    Execute a function with automatic retries upon exception.

    Parameters
    ----------
    func : Callable
        The function to be executed.
    *args :
        Positional arguments forwarded to the function.
    retries : int, optional
        Maximum number of attempts. Default is 3.
    delay : float, optional
        Initial delay (seconds) before retrying. Default is 0.5.
    backoff : float, optional
        Multiplicative factor for exponential backoff.
        If 1.0, the delay remains constant.
    exceptions : iterable of Exception types, optional
        Exception types that should trigger a retry.
    **kwargs :
        Keyword arguments forwarded to the function.

    Returns
    -------
    Any
        The return value of the function if it succeeds.

    Raises
    ------
    Exception
        Re-raises the final exception after exhausting retries.
    """
    attempt = 0
    current_delay = delay

    while True:
        try:
            return func(*args, **kwargs)
        except exceptions as e:
            logger = logging.getLogger(__name__)
            logger.warning(f"Attempt {attempt} failed, will retry in {current_delay} seconds")
            logger.exception(f"Attempt {attempt} failed with exception: {str(e)}")
            attempt += 1
            if attempt > retries:
                raise e
            time.sleep(current_delay)
            current_delay *= backoff
