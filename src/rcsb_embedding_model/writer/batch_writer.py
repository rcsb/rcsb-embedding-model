import os, gzip, tempfile
import torch
import pandas as pd

from abc import abstractmethod
from collections import deque
from abc import ABC
from io import BytesIO

from lightning.pytorch.callbacks import BasePredictionWriter
from rcsb_embedding_model.utils.data import run_with_retries


class CoreBatchWriter(BasePredictionWriter, ABC):
    def __init__(
            self,
            output_path,
            postfix,
            write_interval="batch"
    ):
        super().__init__(write_interval)
        self.out_path = output_path
        self.postfix = postfix

    def write_on_batch_end(
            self,
            trainer,
            pl_module,
            prediction,
            batch_indices,
            batch,
            batch_idx,
            dataloader_idx
    ):
        if prediction is None:
            return
        embeddings, dom_ids = prediction
        deque(map(
            self._write_embedding,
            embeddings,
            dom_ids
        ))

    def file_name(self, dom_id):
        return f'{self.out_path}/{dom_id}.{self.postfix}'

    @abstractmethod
    def _write_embedding(self, embedding, dom_id):
        pass


class CsvBatchWriter(CoreBatchWriter, ABC):
    def __init__(
            self,
            output_path,
            postfix="csv",
            write_interval="batch"
    ):
        super().__init__(output_path, postfix, write_interval)

    def _write_embedding(self, embedding, dom_id):
        pd.DataFrame(embedding.to('cpu').numpy()).to_csv(
            self.file_name(dom_id),
            index=False,
            header=False
        )


class TensorBatchWriter(CoreBatchWriter, ABC):
    def __init__(
            self,
            output_path,
            postfix="pt",
            write_interval="batch",
            device="cpu"
    ):
        super().__init__(output_path, postfix, write_interval)
        self.device = device

    def _write_embedding(self, embedding, dom_id):
        torch.save(
            embedding.to(self.device),
            self.file_name(dom_id)
        )


class DataFrameStorage(CoreBatchWriter, ABC):
    def __init__(
            self,
            output_path,
            df_id,
            postfix="pkl",
            write_interval="batch"
    ):
        super().__init__(output_path, postfix, write_interval)
        self.df_id = df_id
        self.embedding = pd.DataFrame(
            data={},
            columns=['id', 'embedding'],
        )

    def _write_embedding(self, embedding, dom_id):
        self.embedding = pd.concat([
            self.embedding,
            pd.DataFrame(
                data={'id': dom_id, 'embedding': [embedding.to('cpu').numpy()]},
                columns=['id', 'embedding'],
            )
        ], ignore_index=True)

    def on_predict_end(self, trainer, pl_module):
        rank = trainer.global_rank
        path = f"{self.out_path}/{self.df_id}-{rank}.pkl.gz"
        def __embedding_to_pickle():
            self.embedding.to_pickle(
                path,
                compression='gzip'
            )
            _verify_gzip(path)
        run_with_retries(
            __embedding_to_pickle,
            retries=3,
            delay=15,
            backoff=2,
            exceptions=(IOError,)
        )


class JsonStorage(DataFrameStorage, ABC):
    def __init__(
            self,
            output_path,
            df_id,
            postfix="json",
            write_interval="batch"
    ):
        super().__init__(output_path, df_id, postfix, write_interval)

    def on_predict_end(self, trainer, pl_module):
        rank = trainer.global_rank
        path = f"{self.out_path}/{self.df_id}-{rank}.json.gz"
        def __embedding_to_json():
            self.embedding.to_json(
                path,
                orient='records',
                compression='gzip'
            )
            _verify_gzip(path)
        run_with_retries(
            __embedding_to_json,
            retries=3,
            delay=15,
            backoff=2,
            exceptions=(IOError,)
        )

# Generated by Junie
# Simple integrity check to ensure gzip files written by subclasses are consistent
def _verify_gzip(file_path: str):
    try:
        with gzip.open(file_path, 'rb') as f:
            # read through the entire stream so CRC/footer is validated
            while f.read(1024 * 1024):
                pass
    except Exception as e:
        raise IOError(f"Corrupted or inconsistent gzip file detected: {file_path}") from e


# Generated by Junie
def atomic_write_json_gz(df, final_path):
    # serialize to memory first so the gzip write is an all-or-nothing file op
    payload = df.to_json(orient='records').encode('utf-8')
    bio = BytesIO()
    with gzip.GzipFile(fileobj=bio, mode='wb', mtime=0) as gz:
        gz.write(payload)
    data = bio.getvalue()

    dir_ = os.path.dirname(final_path)
    fd, tmp_path = tempfile.mkstemp(dir=dir_, suffix='.tmp')
    try:
        with os.fdopen(fd, 'wb', closefd=True) as f:
            f.write(data)
            f.flush()
            os.fsync(f.fileno())
        os.replace(tmp_path, final_path)  # atomic on same filesystem
    finally:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass